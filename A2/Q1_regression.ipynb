{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import house_sales_data as hsd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['price', 'area', 'beds', 'baths', 'condo', 'location'])\n"
     ]
    }
   ],
   "source": [
    "## Sacramento house price data; load\n",
    "data = hsd.house_sales_data()\n",
    "print(data.keys())\n",
    "\n",
    "## Get the target (i.e., price) and two features\n",
    "price = data[\"price\"]\n",
    "beds = data[\"beds\"]\n",
    "area = data[\"area\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement gradient descent (NOT SGD, just simple GD!)\n",
    "def gradient_descent(X, y, max_iteration, learning_rate):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    costs = np.zeros(max_iteration)\n",
    "    alpha=learning_rate\n",
    "    for i in range(max_iteration):\n",
    "        grad=np.zeros(n)\n",
    "        for j in range(m):\n",
    "            y_hat=X[j]@theta\n",
    "            costs[i]+=1/m*(y_hat-y[j])**2\n",
    "            for k in range(n):\n",
    "                grad[k]+=1/m*2*(y_hat-y[j])*X[j,k]\n",
    "        theta -=alpha*grad\n",
    "    return theta,costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data \n",
    "def normalize_data(X, y, normalize_cols):\n",
    "    min_X = X[:,normalize_cols].min(axis=0)\n",
    "    max_X = X[:,normalize_cols].max(axis=0)\n",
    "    min_y = y.min()\n",
    "    max_y = y.max()\n",
    "    X[:,normalize_cols] = (X[:,normalize_cols] - min_X) / (max_X - min_X)\n",
    "    y[:] = (y - min_y) / (max_y - min_y)\n",
    "    return X,y,min_X, max_X, min_y, max_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement an evaluation metric for regression\n",
    "def evaluation_metric(y,y_hat):\n",
    "    return np.sqrt(np.square(np.subtract(y,y_hat).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data this way\n",
    "X_train = np.hstack((area[:,np.newaxis], beds[:,np.newaxis], np.ones(area.shape[0])[:,np.newaxis]))\n",
    "y_train = price\n",
    "X, y, min_X, max_X, min_y, max_y = normalize_data(X_train, y_train, [True, True, False])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.20860415, 0.14874024, 0.15101251]),\n",
       " array([0.10482456, 0.1008441 , 0.0970652 , 0.09347762, 0.09007163,\n",
       "        0.08683802, 0.08376801, 0.08085329, 0.07808596, 0.07545855,\n",
       "        0.07296392, 0.07059534, 0.0683464 , 0.06621101, 0.06418341,\n",
       "        0.06225812, 0.06042993, 0.05869392, 0.0570454 , 0.05547991,\n",
       "        0.05399325, 0.05258141, 0.05124057, 0.04996714, 0.04875769,\n",
       "        0.04760897, 0.04651789, 0.04548152, 0.04449709, 0.04356194,\n",
       "        0.04267359, 0.04182965, 0.04102786, 0.04026608, 0.03954229,\n",
       "        0.03885454, 0.03820102, 0.03757997, 0.03698975, 0.0364288 ,\n",
       "        0.03589563, 0.03538882, 0.03490704, 0.03444901, 0.03401354,\n",
       "        0.03359947, 0.03320571, 0.03283124, 0.03247508, 0.03213629,\n",
       "        0.031814  , 0.03150736, 0.03121559, 0.03093792, 0.03067365,\n",
       "        0.03042209, 0.03018259, 0.02995455, 0.02973738, 0.02953053,\n",
       "        0.02933348, 0.02914573, 0.02896681, 0.02879627, 0.02863369,\n",
       "        0.02847865, 0.02833079, 0.02818974, 0.02805514, 0.02792668,\n",
       "        0.02780405, 0.02768694, 0.02757508, 0.0274682 , 0.02736605,\n",
       "        0.02726839, 0.027175  , 0.02708565, 0.02700015, 0.0269183 ,\n",
       "        0.0268399 , 0.0267648 , 0.02669282, 0.02662381, 0.02655761,\n",
       "        0.02649409, 0.0264331 , 0.02637452, 0.02631824, 0.02626412,\n",
       "        0.02621207, 0.02616198, 0.02611375, 0.02606729, 0.02602251,\n",
       "        0.02597932, 0.02593765, 0.02589741, 0.02585855, 0.02582098,\n",
       "        0.02578464, 0.02574947, 0.02571542, 0.02568242, 0.02565043,\n",
       "        0.02561939, 0.02558926, 0.02555999, 0.02553154, 0.02550386,\n",
       "        0.02547693, 0.0254507 , 0.02542514, 0.02540021, 0.02537588,\n",
       "        0.02535213, 0.02532893, 0.02530624, 0.02528405, 0.02526233,\n",
       "        0.02524106, 0.0252202 , 0.02519976, 0.0251797 , 0.02516   ,\n",
       "        0.02514065, 0.02512164, 0.02510294, 0.02508453, 0.02506642,\n",
       "        0.02504858, 0.025031  , 0.02501366, 0.02499656, 0.02497969,\n",
       "        0.02496303, 0.02494658, 0.02493032, 0.02491424, 0.02489834,\n",
       "        0.02488261, 0.02486705, 0.02485163, 0.02483637, 0.02482125,\n",
       "        0.02480626, 0.02479139, 0.02477666, 0.02476203, 0.02474753,\n",
       "        0.02473313, 0.02471883, 0.02470463, 0.02469053, 0.02467651,\n",
       "        0.02466259, 0.02464875, 0.02463498, 0.0246213 , 0.02460769,\n",
       "        0.02459415, 0.02458068, 0.02456727, 0.02455393, 0.02454065,\n",
       "        0.02452743, 0.02451426, 0.02450115, 0.02448809, 0.02447508,\n",
       "        0.02446213, 0.02444922, 0.02443635, 0.02442354, 0.02441076,\n",
       "        0.02439803, 0.02438533, 0.02437268, 0.02436006, 0.02434749,\n",
       "        0.02433495, 0.02432244, 0.02430997, 0.02429753, 0.02428512,\n",
       "        0.02427275, 0.02426041, 0.0242481 , 0.02423581, 0.02422356,\n",
       "        0.02421134, 0.02419914, 0.02418697, 0.02417482, 0.02416271,\n",
       "        0.02415062, 0.02413855, 0.02412651, 0.02411449, 0.0241025 ,\n",
       "        0.02409053, 0.02407859, 0.02406666, 0.02405476, 0.02404289,\n",
       "        0.02403103, 0.0240192 , 0.02400738, 0.02399559, 0.02398382,\n",
       "        0.02397207, 0.02396034, 0.02394863, 0.02393694, 0.02392528,\n",
       "        0.02391363, 0.023902  , 0.02389039, 0.02387879, 0.02386722,\n",
       "        0.02385567, 0.02384413, 0.02383262, 0.02382112, 0.02380964,\n",
       "        0.02379818, 0.02378674, 0.02377531, 0.02376391, 0.02375252,\n",
       "        0.02374115, 0.02372979, 0.02371846, 0.02370714, 0.02369584,\n",
       "        0.02368455, 0.02367328, 0.02366203, 0.0236508 , 0.02363958,\n",
       "        0.02362839, 0.0236172 , 0.02360604, 0.02359489, 0.02358376,\n",
       "        0.02357264, 0.02356154, 0.02355046, 0.02353939, 0.02352834,\n",
       "        0.02351731, 0.02350629, 0.02349529, 0.0234843 , 0.02347333,\n",
       "        0.02346238, 0.02345145, 0.02344052, 0.02342962, 0.02341873,\n",
       "        0.02340786, 0.023397  , 0.02338616, 0.02337533, 0.02336452,\n",
       "        0.02335373, 0.02334295, 0.02333219, 0.02332144, 0.02331071,\n",
       "        0.02329999, 0.02328929, 0.0232786 , 0.02326793, 0.02325728,\n",
       "        0.02324664, 0.02323602, 0.02322541, 0.02321481, 0.02320424,\n",
       "        0.02319367, 0.02318312, 0.02317259, 0.02316207, 0.02315157,\n",
       "        0.02314108, 0.02313061, 0.02312016, 0.02310971, 0.02309929,\n",
       "        0.02308887, 0.02307848, 0.02306809, 0.02305773, 0.02304737,\n",
       "        0.02303704, 0.02302671, 0.0230164 , 0.02300611, 0.02299583,\n",
       "        0.02298557, 0.02297532, 0.02296508, 0.02295486, 0.02294466,\n",
       "        0.02293447, 0.02292429, 0.02291413, 0.02290398, 0.02289385,\n",
       "        0.02288373, 0.02287362, 0.02286353, 0.02285346, 0.0228434 ,\n",
       "        0.02283335, 0.02282332, 0.0228133 , 0.0228033 , 0.02279331,\n",
       "        0.02278333, 0.02277337, 0.02276342, 0.02275349, 0.02274357,\n",
       "        0.02273367, 0.02272378, 0.0227139 , 0.02270404, 0.02269419,\n",
       "        0.02268436, 0.02267454, 0.02266473, 0.02265494, 0.02264516,\n",
       "        0.0226354 , 0.02262565, 0.02261591, 0.02260619, 0.02259648,\n",
       "        0.02258678, 0.0225771 , 0.02256743, 0.02255778, 0.02254814,\n",
       "        0.02253851, 0.0225289 , 0.0225193 , 0.02250971, 0.02250014,\n",
       "        0.02249058, 0.02248104, 0.02247151, 0.02246199, 0.02245249,\n",
       "        0.022443  , 0.02243352, 0.02242405, 0.0224146 , 0.02240517,\n",
       "        0.02239574, 0.02238633, 0.02237694, 0.02236755, 0.02235818,\n",
       "        0.02234882, 0.02233948, 0.02233015, 0.02232083, 0.02231153,\n",
       "        0.02230224, 0.02229296, 0.0222837 , 0.02227444, 0.02226521,\n",
       "        0.02225598, 0.02224677, 0.02223757, 0.02222838, 0.02221921,\n",
       "        0.02221005, 0.0222009 , 0.02219177, 0.02218265, 0.02217354,\n",
       "        0.02216444, 0.02215536, 0.02214629, 0.02213723, 0.02212819,\n",
       "        0.02211916, 0.02211014, 0.02210113, 0.02209214, 0.02208316,\n",
       "        0.02207419, 0.02206523, 0.02205629, 0.02204736, 0.02203844,\n",
       "        0.02202954, 0.02202065, 0.02201177, 0.0220029 , 0.02199405,\n",
       "        0.0219852 , 0.02197637, 0.02196756, 0.02195875, 0.02194996,\n",
       "        0.02194118, 0.02193241, 0.02192366, 0.02191491, 0.02190618,\n",
       "        0.02189746, 0.02188876, 0.02188007, 0.02187138, 0.02186271,\n",
       "        0.02185406, 0.02184541, 0.02183678, 0.02182816, 0.02181955,\n",
       "        0.02181096, 0.02180237, 0.0217938 , 0.02178524, 0.02177669,\n",
       "        0.02176816, 0.02175963, 0.02175112, 0.02174262, 0.02173413,\n",
       "        0.02172566, 0.02171719, 0.02170874, 0.0217003 , 0.02169187,\n",
       "        0.02168345, 0.02167505, 0.02166666, 0.02165828, 0.02164991,\n",
       "        0.02164155, 0.0216332 , 0.02162487, 0.02161655, 0.02160824,\n",
       "        0.02159994, 0.02159165, 0.02158337, 0.02157511, 0.02156686,\n",
       "        0.02155861, 0.02155039, 0.02154217, 0.02153396, 0.02152577,\n",
       "        0.02151758, 0.02150941, 0.02150125, 0.0214931 , 0.02148496,\n",
       "        0.02147684, 0.02146872, 0.02146062, 0.02145253, 0.02144445,\n",
       "        0.02143638, 0.02142832, 0.02142027, 0.02141224, 0.02140421,\n",
       "        0.0213962 , 0.0213882 , 0.02138021, 0.02137223, 0.02136426,\n",
       "        0.0213563 , 0.02134836, 0.02134042, 0.0213325 , 0.02132458,\n",
       "        0.02131668, 0.02130879, 0.02130091, 0.02129304, 0.02128519,\n",
       "        0.02127734, 0.0212695 , 0.02126168, 0.02125387, 0.02124606,\n",
       "        0.02123827, 0.02123049, 0.02122272, 0.02121496, 0.02120721]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call gradient descent and fine-tune your learning rate\n",
    "gradient_descent(X_train,y_train,500,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efc092d9460>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot cost vs iterations \n",
    "# note gradient descent return costs for all iterations\n",
    "theta_for500,costs=gradient_descent(X_train,y_train,500,0.01)\n",
    "plt.plot(costs,'r.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010036888206451202\n",
      "0.006933272329693822\n"
     ]
    }
   ],
   "source": [
    "# print evaluation metric for test set while\n",
    "# changing max_iteration from 500 to 2000\n",
    "\n",
    "##### for 500\n",
    "theta_for500,costs=gradient_descent(X_train,y_train,500,0.01)\n",
    "y_test_hat500=X_test@theta_for500\n",
    "print(evaluation_metric(y_test,y_test_hat500))\n",
    "\n",
    "########## for 2000\n",
    "theta_for2000,costs=gradient_descent(X_train,y_train,2000,0.01)\n",
    "y_test_hat2000=X_test@theta_for2000\n",
    "print(evaluation_metric(y_test,y_test_hat2000))\n",
    "\n",
    "# Note the best value of evaluation metric you get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best evaluation metric is for 2000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare your solution with scikit-learn library\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using the training sets\n",
    "LR= LinearRegression()\n",
    "LR.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006546762800805023"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on test set and print evaluation metric\n",
    "predicted_data=LR.predict(X_test)\n",
    "predicted_data\n",
    "evaluation_metric(y_test,predicted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003865095288887947"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much does it differ by from \n",
    "evaluation_metric(y_test_hat2000,predicted_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
