{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "instrumental-hypothetical",
   "metadata": {},
   "source": [
    "\n",
    "## Theorem: Prove that under Gaussian noise assumption linear regression amounts to least square.\n",
    "\n",
    "### Proof: \n",
    "Let us assume that the target variables and the inputs are related via the\n",
    "equation \n",
    "$$y_i=\\theta^{T} x_i \\ +\\ \\epsilon_i\\ \\ \\ \\mbox{for}\\ i=1 \\left(1\\right)\\ m$$ \n",
    "where $ \\epsilon_i $ is an error term.\n",
    "\n",
    "Let us further assume that the  $\\epsilon_i$ are distributed <!-- [Text](link) -->\n",
    "[IID](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) according to a <!-- [Text](link) -->\n",
    "[Gaussian distribution](https://en.wikipedia.org/wiki/Normal_distribution) with mean zero and some variance $\\sigma$ i.e., $$\\epsilon_i\\ \\sim \\ \\mathcal{N}(0,\\sigma^2)$$\n",
    "i.e., the density of $\\epsilon_i$ is given by\n",
    "$$p(\\epsilon_i)\\ =\\ \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{\\epsilon_i^2}{2\\sigma^{2}}\\right)$$\n",
    "\n",
    "This implies that $$p(y_i|x_i;\\theta)\\ =\\ \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y_i-\\theta^{T} x_i)^2}{2\\sigma^{2}}\\right)$$\n",
    "To check the distribution of $y_i$'s given $x_i$'s for a fixed value $\\theta$ we have the <!-- [Text](link) -->\n",
    "[likelihood function](https://en.wikipedia.org/wiki/Likelihood_function)\n",
    "\n",
    "\\begin{aligned}\n",
    "\\textbf{L}(\\theta)\\ \n",
    "&= \\ p(\\vec{y}|X;\\theta)\\\\\n",
    "&= \\prod_{i=1}^{m}p(y_i|x_i;\\theta)\\\\\n",
    "&=\\ \\prod_{i=1}^{m}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y_i-\\theta^{T} x_i)^2}{2\\sigma^{2}}\\right)\n",
    "\\end{aligned}\n",
    "\n",
    "In order to make the data as high probability as possible we have to maximize L($\\theta$) over $\\theta$.\n",
    "\\begin{aligned}\n",
    "Let \\ \\ell(\\theta) =\\ \\log\\textbf{L}(\\theta) \\\\ \\\\\n",
    "&\\underset{\\theta}{\\arg\\max} \\ \\ell(\\theta)\\\\ \n",
    "&=\\underset{\\theta}{\\arg\\max} \\ \\log\\textbf{L}(\\theta) \\\\\n",
    "&=\\underset{\\theta}{\\arg\\max} \\left( \\  \\log\\prod_{i=1}^{m}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y_i-\\theta^{T} x_i)^2}{2\\sigma^{2}}\\right)\\right) \\\\\n",
    "&=\\underset{\\theta}{\\arg\\max} \\ \\left(\\sum_{i=1}^{m}\\log \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y_i-\\theta^{T} x_i)^2}{2\\sigma^{2}}\\right)\\right)\\\\\n",
    "&=\\underset{\\theta}{\\arg\\max} \\ \\left(m\\log\\frac{1}{\\sqrt{2\\pi}\\sigma}-\\frac{1}{\\sigma^2}*\\frac{1}{2}\\sum_{i=1}^{m}(y_i-\\theta^{T} x_i)^2\\right) \\\\\n",
    "&=\\underset{\\theta}{\\arg\\max} \\ \\left(-\\frac{1}{\\sigma^2}*\\frac{1}{2}\\sum_{i=1}^{m}(y_i-\\theta^{T} x_i)^2\\right)\n",
    "\\end{aligned}\n",
    "\n",
    "Hence, maximizing  $\\ell(\\theta)$ gives the same answer as minimizing  $\\frac{1}{2}\\sum_{i=1}^{m}(y_i-\\theta^{T} x_i)^2$ which we recognize to be J($\\theta$), our original least-squares cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-attempt",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
